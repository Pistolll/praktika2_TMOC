{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pistolll/praktika2_TMOC/blob/main/1.1_prak_02_NB_SpamEmail_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GAN0G-6jrcpU",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94cc030f-3e95-46b3-85e1-d3e9e11616da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Импорт основных библиотек для работы с данными\n",
        "import numpy as np  # Для численных операций и работы с массивами\n",
        "import pandas as pd  # Для работы с табличными данными (DataFrame)\n",
        "\n",
        "# Импорт библиотек для визуализации\n",
        "import matplotlib as mpl  # Базовый функционал matplotlib\n",
        "import matplotlib.pyplot as plt  # Построение графиков\n",
        "import seaborn as sns  # Улучшенные визуализации на основе matplotlib\n",
        "\n",
        "\n",
        "# Импорт модулей для работы с текстом\n",
        "import string  # Работа со строковыми операциями\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Преобразование текста в TF-IDF признаки\n",
        "from sklearn.feature_extraction.text import CountVectorizer  # Преобразование текста в мешок слов\n",
        "\n",
        "# Импорт инструментов машинного обучения\n",
        "from sklearn.model_selection import train_test_split  # Разделение данных на обучающую/тестовую выборки\n",
        "from sklearn import preprocessing  # Предварительная обработка данных\n",
        "\n",
        "# Импорт NLP-инструментов из NLTK\n",
        "from nltk.stem import SnowballStemmer  # Стеммер для приведения слов к основе\n",
        "from nltk.tokenize import RegexpTokenizer  # Токенизатор по регулярным выражениям\n",
        "from nltk.corpus import stopwords  # Список стоп-слов\n",
        "from nltk.stem.porter import *  # Поттер-стеммер\n",
        "import nltk  # Основная NLP-библиотека\n",
        "nltk.download('stopwords')  # Загрузка стоп-слов для NLTK\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JYfWY6BosKQ6",
        "tags": []
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/Pistolll/praktika2_TMOC/refs/heads/main/%D0%BF%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D0%BA%D0%B0%202/01-custom-naive-bayes/spam.csv\"\n",
        "df = pd.read_csv(url, encoding='ISO-8859-1')\n",
        "le = preprocessing.LabelEncoder() # категориальные метки в числовой формат"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "q3Kxk8x9sdXj",
        "tags": []
      },
      "outputs": [],
      "source": [
        "data = df.to_numpy() # DataFrame df в массив NumPy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CQ7z-5n9sge9",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Выделяем признаки (X) и целевую переменную (y) из данных:\n",
        "X = data[:, 1]  # Берём все строки и только второй столбец (индекс 1) - это матрица признаков\n",
        "y = data[:, 0]  # Берём все строки и первый столбец (индекс 0) - это вектор целевых значений"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCIVTx6Ysggm",
        "outputId": "8ec950ac-e044-4d3d-d728-12b56e9b6979",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5572,), (5572,))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "e7ZpFSZvsgqE",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Инициализация токенизатора для разбиения текста на токены\n",
        "# RegexpTokenizer('\\w+') - разделяет текст только на слова (игнорирует пунктуацию)\n",
        "# \\w+ - регулярное выражение, означающее \"1 или более буквенно-цифровых символов\"\n",
        "tokenizer = RegexpTokenizer('\\w+')\n",
        "\n",
        "# Стоп-слова - это частые, но малозначимые слова (артикли, предлоги и т.д.)\n",
        "# set() преобразует список в множество для быстрого поиска\n",
        "sw = set(stopwords.words('english'))\n",
        "\n",
        "# Стеммер приводит слова к их корневой форме (например: \"running\" → \"run\")\n",
        "ps = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KfOsXqgcsgtq",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def getStem(review):\n",
        "\n",
        "    # Приводим весь текст к нижнему регистру для единообразия\n",
        "    review = review.lower()\n",
        "\n",
        "    # Разбиваем текст на отдельные слова (токены), игнорируя пунктуацию\n",
        "    tokens = tokenizer.tokenize(review)\n",
        "\n",
        "    # Удаляем стоп-слова (малозначимые слова, предлоги, артикли и т.д.)\n",
        "    removed_stopwords = [w for w in tokens if w not in sw]\n",
        "\n",
        "    # Применяем стемминг к каждому слову (приводим к корневой форме)\n",
        "    stemmed_words = [ps.stem(token) for token in removed_stopwords]\n",
        "\n",
        "    # Собираем обработанные слова обратно в единую строку\n",
        "    clean_review = ' '.join(stemmed_words)\n",
        "\n",
        "    return clean_review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PjtP4dNGsgxj",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Функция для очистки и предварительной обработки набора текстовых документов. Возвращает список обработанных и очищенных документов\n",
        "def getDoc(document):\n",
        "    d = []  # Создаем пустой список для хранения обработанных документов\n",
        "\n",
        "    # Проходим по каждому документу в переданной коллекции\n",
        "    for doc in document:\n",
        "        # Применяем функцию getStem для очистки и обработки каждого документа\n",
        "        d.append(getStem(doc))\n",
        "\n",
        "    return d  # Возвращаем список обработанных документов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UiD7bPEfsgo8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Получаем очищенные документы, применяя функцию getDoc к массиву X\n",
        "stemmed_doc = getDoc(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6X2GSJXsgmq",
        "outputId": "24c7fcff-b506-4b72-852b-48a959fa6e3d",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['go jurong point crazi avail bugi n great world la e buffet cine got amor wat',\n",
              " 'ok lar joke wif u oni',\n",
              " 'free entri 2 wkli comp win fa cup final tkt 21st may 2005 text fa 87121 receiv entri question std txt rate c appli 08452810075over18',\n",
              " 'u dun say earli hor u c alreadi say',\n",
              " 'nah think goe usf live around though',\n",
              " 'freemsg hey darl 3 week word back like fun still tb ok xxx std chg send ã â 1 50 rcv',\n",
              " 'even brother like speak treat like aid patent',\n",
              " 'per request mell mell oru minnaminungint nurungu vettam set callertun caller press 9 copi friend callertun',\n",
              " 'winner valu network custom select receivea ã â 900 prize reward claim call 09061701461 claim code kl341 valid 12 hour',\n",
              " 'mobil 11 month u r entitl updat latest colour mobil camera free call mobil updat co free 08002986030']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Выводим первые 10 очищенных и стеммированных документов\n",
        "stemmed_doc[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "p17dxL_zsgkp",
        "tags": []
      },
      "outputs": [],
      "source": [
        "cv = CountVectorizer() # преобразует коллекцию текстовых документов в числовую матрицу, где строки представляют документы,\n",
        "# а столбцы — уникальные слова (или токены) из этих документов. Каждый элемент матрицы указывает, сколько раз соответствующее слово встречается в документе."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3xEOvVs-s3Uz",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Создание словаря с помощью CountVectorizer\n",
        "vc = cv.fit_transform(stemmed_doc)\n",
        "#позволяет определить уникальные слова, которые будут использоваться для представления текстовых данных в числовом формате."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "jIYvFfTNs5sA",
        "tags": []
      },
      "outputs": [],
      "source": [
        "X = vc.todense() # преобразует матрицу, созданную с помощью CountVectorizer, в плотную матрицу"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "xSNI8pdjs5uF",
        "tags": []
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # разделяем на обучающую и тестовую выборки, 33% данных будут отведены для тестирования, а оставшиеся 67% — для обучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "SNFPDN0Vs53j",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Импортирование класса MultinomialNB из библиотеки sklearn\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "# модель наивного байесовского классификатора, что позволит классифицировать новые документы на основе обученных данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOsrenTNs57y",
        "outputId": "df3b3708-ba64-4b20-9088-275730e757f9",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.977705274605764"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# Создание экземпляра модели наивного байесовского классификатора\n",
        "model = MultinomialNB()\n",
        "\n",
        "# Обучение модели на обучающем наборе данных\n",
        "model.fit(np.asarray(X_train), y_train)\n",
        "\n",
        "# Оценка точности модели на тестовом наборе данных\n",
        "model.score(np.asarray(X_test), y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "n8-Rtl_Ys513",
        "tags": []
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    \"\"\"\n",
        "    Hi Kunal,\n",
        "We invite you to participate in MishMash - India’s largest online diversity hackathon.\n",
        "The hackathon is a Skillenza initiative and sponsored by Microsoft, Unity, Unilever, Gojek, Rocketium and Jharkhand Government.\n",
        "We have a special theme for you - Deep Tech/Machine Learning - sponsored by Unilever, which will be perfect for you.\n",
        "    \"\"\",\n",
        "    \"\"\"Join us today at 12:00 PM ET / 16:00 UTC for a Red Hat DevNation tech talk on AWS Lambda and serverless Java with Bill Burke.\n",
        "Have you ever tried Java on AWS Lambda but found that the cold-start latency and memory usage were far too high?\n",
        "In this session, we will show how we optimized Java for serverless applications by leveraging GraalVM with Quarkus to\n",
        "provide both supersonic startup speed and a subatomic memory footprint.\"\"\",\n",
        "\n",
        "    \"\"\"We really appreciate your interest and wanted to let you know that we have received your application.\n",
        "There is strong competition for jobs at Intel, and we receive many applications. As a result, it may take some time to get back to you.\n",
        "Whether or not this position ends up being a fit, we will keep your information per data retention policies,\n",
        "so we can contact you for other positions that align to your experience and skill set.\n",
        "\"\"\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "SvViQfDvs5zX",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#преобразовать обработанные документы в матрицу частот слов.\n",
        "def prepare(messages):\n",
        "    d = getDoc(messages)\n",
        "    # dont do fit_transform!! it will create new vocab.\n",
        "    return cv.transform(d)\n",
        "messages = prepare(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu76hOSPs5x2",
        "outputId": "e5ea0143-621b-4630-e9bf-c0857e8c1331",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ham', 'spam', 'ham'], dtype='<U4')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# В этом коде используется метод predict модели model,\n",
        "# чтобы сделать предсказания на основе подготовленных сообщений, которые были переданы в функцию prepare. Это позволяет модели классифицировать каждое сообщение в соответствии с обученными категориями\n",
        "y_pred = model.predict(messages)\n",
        "y_pred"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}